{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dab9023",
   "metadata": {},
   "source": [
    "## Loading YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd1ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt'}, page_content=\"00:00:00.000 Hey team, Justin Zeltzer here from\\xa0zstatistics.com, where today I'm\\xa0\\xa0\\n00:00:05.430 responding to a challenge that was issued to me.\\xa0Someone asked me if I could explain statistics\\xa0\\xa0\\n00:00:11.160 to them in under half an hour. While initially\\xa0I thought that was a bit of an ambitious ask,\\xa0\\xa0\\n00:00:17.250 I thought no that's actually a really good\\xa0challenge, and one that I might do for everybody.\\xa0\\xa0\\n00:00:23.280 So this is it! An introduction to statistics,\\xa0with no maths, and done in under half an hour.\\xa0\\xa0\\n00:00:30.300 Now you can probably see that the the timing of\\xa0this video is a bit longer than that, but it is\\xa0\\xa0\\n00:00:36.480 because I bunged on a little extra section at\\xa0the end- which is a bit of an optional extra,\\xa0\\xa0\\n00:00:41.220 but I think I get most of it done in under half\\xa0an hour. But the idea is for you to develop your\\xa0\\xa0\\n00:00:47.280 intuition around statistics, so it's great\\xa0for those people who are just enrolling in a\\xa0\\xa0\\n00:00:52.080 statistics course and are a bit apprehensive,\\xa0or for others who aren't studying statistics,\\xa0\\xa0\\n00:00:58.110 but kind of want to know what it's all about. And\\xa0to keep it light and interesting, I've themed all\\xa0\\xa0\\n00:01:04.290 of the examples in this video on my latest\\xa0obsession, which is the NBA. Despite proudly\\xa0\\xa0\\n00:01:11.310 following Australian sports my brother's getting\\xa0me hopelessly addicted to American basketball\\n00:01:19.050 So here we go, the first thing we're going to\\xa0delve into is what types of data we're going\\xa0\\xa0\\n00:01:23.700 to encounter when we're dealing with statistics.\\xa0Now roughly we can divide data into two distinct\\xa0\\xa0\\n00:01:31.500 classes: categorical data and numerical data.\\xa0Now they sound somewhat self-explanatory because\\xa0\\xa0\\n00:01:40.530 numerical means numbers and categorical means\\xa0categories, so I'll give you some examples of\\xa0\\xa0\\n00:01:47.880 those in a second, but categorical data can be\\xa0further split into nominal categorical data and\\xa0\\xa0\\n00:01:54.480 ordinal categorical data. Nominal meaning\\xa0there is no order to the various categories\\xa0\\xa0\\n00:02:02.490 of a particular variable and ordinal means that\\xa0there is some kind of order to the categories,\\xa0\\xa0\\n00:02:09.600 and we'll see some examples in a second. Numerical\\xa0data can be further split into Discrete numerical\\xa0\\xa0\\n00:02:16.170 data or Continuous numerical data, and again\\xa0we'll have a look at some examples right now.\\xa0\\xa0\\n00:02:22.740 So if I was to ask you what team does Steph Curry\\xa0play for, you can see that clearly the answer to\\xa0\\xa0\\n00:02:31.530 that question is not going to be numerical, so\\xa0it's a categorical piece of data. And here in\\xa0\\xa0\\n00:02:40.260 these brackets I've put what's called the sample\\xa0space for this particular question. Now Steph\\xa0\\xa0\\n00:02:47.940 Curry could either play for the Atlanta Hawks, the\\xa0Boston Celtics etc etc. It turns out he plays for\\xa0\\xa0\\n00:02:54.840 the Golden State Warriors, but this but all these\\xa0potential values for what team Steph Curry plays\\xa0\\xa0\\n00:03:01.380 for when we combine them we call that the sample\\xa0space and you can see that there's no order to the\\xa0\\xa0\\n00:03:08.400 teams. It doesn't really matter which order you\\xa0put them in, so that's why we would say this is\\xa0\\xa0\\n00:03:12.330 a nominal piece of data. Now the question: what\\xa0position does Steph play? Well that might provide\\xa0\\xa0\\n00:03:20.940 us with ordinal categorical data. Now Steph could\\xa0either play Guard, Forward or Center, or you\\xa0\\xa0\\n00:03:28.260 could split that up into Point Guard, Shooting\\xa0Guard etc etc, but there is some loose order to\\xa0\\xa0\\n00:03:35.160 these positions. The Guards generally play in the\\xa0back court and then the forwards played closer to\\xa0\\xa0\\n00:03:39.990 the ring and the center plays underneath it. And\\xa0also there's a general kind of height difference\\xa0\\xa0\\n00:03:44.460 between smaller players that play guard to taller\\xa0players playing forward and the tallest will play\\xa0\\xa0\\n00:03:51.210 at center so while this is still categorical data,\\xa0there's some kind of order to it. Now an example\\xa0\\xa0\\n00:04:02.430 of discrete numerical data might be how many free\\xa0throws has Steph missed tonight. Clearly he could\\xa0\\xa0\\n00:04:08.970 miss 0 1 2 3 etc etc. This is numerical data, but\\xa0importantly he can't miss 1.5 or 2.3 free throws,\\xa0\\xa0\\n00:04:21.180 right? So there are only discrete possible\\xa0values that this piece of data can take. And\\xa0\\xa0\\n00:04:30.390 finally continuous data might be the question:\\xa0What's Steph's height? Now google lists Steph's\\xa0\\xa0\\n00:04:38.880 height as 191 centimeters but of course Steph's\\xa0actual height might be something like 191.3 or .2\\xa0\\xa0\\n00:04:48.000 .1. 7. You can keep subdividing these centimeters\\xa0into as many decimal places as you as you like,\\xa0\\xa0\\n00:04:55.320 so height in this instance is an example of a\\xa0continuous numerical piece of data. Generally\\xa0\\xa0\\n00:05:04.230 we kind of make height a discrete piece of data\\xa0because we only really are interested in whole\\xa0\\xa0\\n00:05:10.020 centimeters or in the case of the Imperial\\xa0measure: whole inches. We don't usually care\\xa0\\xa0\\n00:05:18.900 if someone's you know six foot three and a half\\xa0or six foot three and two thirds. But in a pure\\xa0\\xa0\\n00:05:24.630 sense, you could say that height is continuous.\\xa0Now here's an interesting question: if I asked\\xa0\\xa0\\n00:05:31.920 you what is Steph's three-point percentage this\\xa0season, what kind of data do you think that is?\\xa0\\xa0\\n00:05:38.550 Is it categorical? Is it numerical? And what kind\\xa0of which of these subcategories would this relate\\xa0\\xa0\\n00:05:45.030 to maybe you want to pause the video and have a\\xa0think, but spoiler alert I'm gonna tell you right\\xa0\\xa0\\n00:05:51.610 now when we have a quick look at what proportions\\xa0in fact are now. Just appreciate the percentages\\xa0\\xa0\\n00:05:56.800 and proportions are pretty much the same thing\\xa0one's just expressed in terms of being out of\\xa0\\xa0\\n00:06:02.350 a hundred and the other one is just a number\\xa0between zero and one but it's the same thing\\xa0\\xa0\\n00:06:08.770 all right now if I was to ask you what is Steph's\\xa0three-point percentage this season what type of\\xa0\\xa0\\n00:06:22.390 data do you think this is. Is it categorical\\xa0or is it numerical and which subcategory do\\xa0\\xa0\\n00:06:29.920 you think it fits into well feel free to pause\\xa0the video and have a think about it but spoiler\\xa0\\xa0\\n00:06:36.820 alert I'm about to ruin it for you when we have a\\xa0look at a special kind of data called proportions.\\n00:06:44.320 Now even though I've asked for Steph's three-point\\xa0percentage, hopefully you could appreciate that a\\xa0\\xa0\\n00:06:50.980 percentage is in fact just a proportion that's\\xa0being expressed out of a hundred but we'll note\\xa0\\xa0\\n00:06:57.790 that each three-point attempt that Steph makes\\xa0actually provides us with nominal data so it's\\xa0\\xa0\\n00:07:04.150 either a three-point that's made or a three-point\\xa0that's missed so what a proportion does is that\\xa0\\xa0\\n00:07:11.110 it aggregates this information to provide a\\xa0numerical summary figure so in some senses\\xa0\\xa0\\n00:07:18.430 a proportion is numerical because obviously it\\xa0provides us with a number but it's built up off\\xa0\\xa0\\n00:07:24.220 nominal data now Steph Curry in this season so\\xa0far in 2018 19 has made 128 three-point shots\\xa0\\xa0\\n00:07:34.000 and this is his percentage point four seven six\\xa0six so each of these 128 shots are actually piece\\xa0\\xa0\\n00:07:41.950 nominal pieces of information this proportion\\xa0is some kind of summary of that now here's an\\xa0\\xa0\\n00:07:48.340 interesting question for you is a proportionate\\xa0and discrete or continuous numerical data and\\xa0\\xa0\\n00:07:54.010 that's not necessarily so obvious a question and I\\xa0might leave that to you to answer in the comments\\xa0\\xa0\\n00:07:58.960 of this video so feel free to start a little\\xa0discussion on that it's a it's an interesting\\xa0\\xa0\\n00:08:03.340 one I think anyway that's your data types now\\xa0distributions if I was to ask how other heights\\xa0\\xa0\\n00:08:12.790 of NBA players distributed now the smallest player\\xa0currently playing in the 2018 19 season is Isaiah\\xa0\\xa0\\n00:08:22.960 Thomas at 5 foot 9 and the largest player is\\xa0bobbin Marjanovic and seven foot three now all\\xa0\\xa0\\n00:08:29.830 the other players will fit somewhere between\\xa0the smallest and largest here we go there's a\\xa0\\xa0\\n00:08:35.559 pickie of both of those two players anyway what\\xa0we can present is something called a probability\\xa0\\xa0\\n00:08:41.500 density function which essentially describes the\\xa0distribution of all the players in between the\\xa0\\xa0\\n00:08:47.920 smallest and largest player here so you can think\\xa0about it in two ways it's either the distribution\\xa0\\xa0\\n00:08:55.360 of the whole population of basketball players\\xa0that we have in the NBA or alternatively it's the\\xa0\\xa0\\n00:09:02.050 probability of selecting someone at random from\\xa0that population at every given height so quite\\xa0\\xa0\\n00:09:09.460 clearly as the bulk of the players are going to\\xa0be somewhere in the middle say six foot six or six\\xa0\\xa0\\n00:09:14.050 foot five or something if I was to select someone\\xa0at random I would have the highest probability of\\xa0\\xa0\\n00:09:20.200 selecting someone around that height as opposed\\xa0to selecting someone at five foot nine or seven\\xa0\\xa0\\n00:09:26.140 foot three there's just less players at those\\xa0Heights right now this curve I'm presenting here\\xa0\\xa0\\n00:09:31.960 is a very common curve in statistics some people\\xa0call it a bell curve other people call a normal\\xa0\\xa0\\n00:09:39.340 distribution but it's a very commonly occurring\\xa0distribution in statistics and it basically just\\xa0\\xa0\\n00:09:44.980 means that the bulk of the distribution happens\\xa0towards the middle and it gets rarer as you go\\xa0\\xa0\\n00:09:51.340 towards the extremes now I've created a whole\\xa0video on the normal distribution which I'll\\xa0\\xa0\\n00:09:57.520 put a little flash hyperlink up now for if you're\\xa0keen on learning a bit more about it but there's\\xa0\\xa0\\n00:10:03.340 a symmetry about this distribution with the bulk\\xa0of the players being around that height now I've\\xa0\\xa0\\n00:10:10.390 just assumed that this would be the distribution\\xa0of basketballers heights but what other possible\\xa0\\xa0\\n00:10:16.930 distributions might there be a distribution like\\xa0this would indicate that it's the same probability\\xa0\\xa0\\n00:10:23.650 that if I was to select someone at random from the\\xa0NBA there'd be the same probability of being six\\xa0\\xa0\\n00:10:30.520 foot six as they would of being five foot nine or\\xa0indeed 743 this is called a uniform distribution\\xa0\\xa0\\n00:10:37.300 which probably doesn't match up with the reality\\xa0of the NBA a distribution like this we can call\\xa0\\xa0\\n00:10:45.010 this a bi modal distribution it's got two modes\\xa0where the mode is just the highest peak of the\\xa0\\xa0\\n00:10:51.730 graph or something like this which is a skewed\\xa0distribution let's just say that there's a larger\\xa0\\xa0\\n00:10:59.140 predominance of players up towards the seven-foot\\xa0mark and it gets I think it's a lot more scarce\\xa0\\xa0\\n00:11:04.420 down towards these smaller players and this\\xa0particular type of skew is actually called left\\xa0\\xa0\\n00:11:11.050 skew because the tail points in the left direction\\xa0you can guess what right askew might look like now\\xa0\\xa0\\n00:11:21.340 before we move on to have a look at sampling\\xa0distributions I just want to reiterate that\\xa0\\xa0\\n00:11:25.810 this distribution we've been looking at describes\\xa0the probability distribution of heights if I was\\xa0\\xa0\\n00:11:31.480 to select one single player but what if I had\\xa0a whole sample of players say ten players and\\xa0\\xa0\\n00:11:39.010 I wanted to know what is the distribution what\\xa0is the probability distribution of their average\\xa0\\xa0\\n00:11:43.810 height or for that we'll be looking at sampling\\xa0distributions so the question is if I select ten\\xa0\\xa0\\n00:11:51.520 players at random what is the probability\\xa0distribution of their average height okay\\xa0\\xa0\\n00:11:57.130 well here's the underlying distribution again now\\xa0there's five foot nine and there's seven for three\\xa0\\xa0\\n00:12:03.070 and if I was to let's select someone at random\\xa0the probability density function would be a bit\\xa0\\xa0\\n00:12:09.400 like that but if I select ten people and take a\\xa0look at their average what will that distribution\\xa0\\xa0\\n00:12:18.040 look like and it turns out that it'll have the\\xa0same mean but it'll be a lot skinnier now why is\\xa0\\xa0\\n00:12:25.120 that well think of it this way if I select someone\\xa0at random it's possible that I select Isiah Thomas\\xa0\\xa0\\n00:12:30.670 he's five foot nine and while there might only be\\xa0a few of him in a league that are that small it's\\xa0\\xa0\\n00:12:37.360 still possible that I select that player at random\\xa0but if I'm selecting ten players the probability\\xa0\\xa0\\n00:12:43.570 of them having an average height of five foot\\xa0nine is very very very small indeed eventually\\xa0\\xa0\\n00:12:51.400 after selecting Isiah Thomas maybe I'll have to\\xa0select other players and it's likely that they're\\xa0\\xa0\\n00:12:56.560 going to be somewhere else in the distribution so\\xa0that their average gets shifted up right when you\\xa0\\xa0\\n00:13:02.560 take a sample the larger your sample size is the\\xa0more unlikely you are to get extreme sample means\\xa0\\xa0\\n00:13:10.300 so that's why this distribution is going to be\\xa0a lot skinnier than the distribution on the left\\xa0\\xa0\\n00:13:18.490 here and this is important in statistics because\\xa0every study that ever gets conducted starts with\\xa0\\xa0\\n00:13:24.940 a sample you want to test some kind of effect so\\xa0you take a sample and then you make an inference\\xa0\\xa0\\n00:13:30.800 using that sample so it's important for us to get\\xa0a handle on what happens when we take a sample the\\xa0\\xa0\\n00:13:37.910 distribution becomes a lot skinnier or in other\\xa0words the variance of our statistic is reduced and\\xa0\\xa0\\n00:13:49.760 that indeed takes us to sampling and estimation my\\xa0question is how good is Steph Curry currently at\\xa0\\xa0\\n00:13:58.400 three-pointers so in the current season 2018-19\\xa0ice he's shot to 128 threes and has nailed 61\\xa0\\xa0\\n00:14:09.110 of them so that's a point for 766 what I'm gonna\\xa0try to get across here is that this is actually\\xa0\\xa0\\n00:14:15.320 a sample statistic he has a sample of size 128 he\\xa0has 128 three-point attempts and 61 of them have\\xa0\\xa0\\n00:14:25.160 been successes so here we have that proportion\\xa0point 4 7 6 6 which is our sample statistic now\\xa0\\xa0\\n00:14:33.710 when I ask you how good is Steph current Steph\\xa0Curry at three-pointers appreciate at this point\\xa0\\xa0\\n00:14:39.350 4 7 6 6 is actually an estimate for this thing\\xa0we're going to call theta now theta is a Greek\\xa0\\xa0\\n00:14:47.240 letter and it represents exactly how good Steph\\xa0Curry is it's something we can never know maybe\\xa0\\xa0\\n00:14:54.950 he's a 50% shooter but this season he's just a\\xa0little bit off or maybe he only shoots at 42% and\\xa0\\xa0\\n00:15:01.130 this season he's doing much better but either way\\xa0what statistics does is it creates this unknowable\\xa0\\xa0\\n00:15:09.410 almost godlike god-like value of theta which we\\xa0can then try to estimate by taking a sample so\\xa0\\xa0\\n00:15:19.250 given our sample where Steph Curry has currently\\xa0got point 4 7 6 6 maybe the best estimate for\\xa0\\xa0\\n00:15:27.110 theta might be point 4 7 6 6 as in if you were\\xa0trying to guess what you think Steph Curry's\\xa0\\xa0\\n00:15:33.830 long-run 3-point percentage would be 0.47 6 6 is\\xa0probably best bet but appreciate that there's some\\xa0\\xa0\\n00:15:42.470 kind of variance around this estimate some kind\\xa0of uncertainty if Steph Curry shoots some more\\xa0\\xa0\\n00:15:49.880 three-pointers this proportion could either go\\xa0up or down right and all of a sudden that would\\xa0\\xa0\\n00:15:55.040 be our new best estimate for theta the whole idea\\xa0behind statistics is trying to get a hold of the\\xa0\\xa0\\n00:16:02.690 uncertainty you have behind your estimates now\\xa0I'm not going to enter into calculations of those\\xa0\\xa0\\n00:16:09.260 of these particular intervals in this video but\\xa0I've made plenty of videos that delve into this\\xa0\\xa0\\n00:16:14.990 precise question but what statisticians like to\\xa0do is create these things called 95% confidence\\xa0\\xa0\\n00:16:23.540 intervals where we can say look we don't know\\xa0what theta is but given our sample we have a 95%\\xa0\\xa0\\n00:16:30.620 confidence that fitas between these two particular\\xa0values and generally our sample estimate is bang\\xa0\\xa0\\n00:16:38.210 in the middle of those two limits so that's\\xa0what you're going to be doing when you study\\xa0\\xa0\\n00:16:44.570 statistics you're going to be developing means\\xa0of calculating of quantifying this uncertainty\\xa0\\xa0\\n00:16:50.720 you have over Steph Curry's long term 3-point\\xa0percentage or other things maybe more meaningful\\xa0\\xa0\\n00:16:57.140 now here's an interesting point we all know that\\xa0Steph Curry is probably the best 3-point shooter\\xa0\\xa0\\n00:17:03.260 in the league if not in basketball history but at\\xa0this point in the 2018 2019 season were only about\\xa0\\xa0\\n00:17:11.990 sort of 12 or 13 games in at this point there's\\xa0a player called Meyers Leonard who scored 9 out\\xa0\\xa0\\n00:17:18.109 of 15 three-pointers and has a three-point\\xa0percentage of 0.6 now who do you think out\\xa0\\xa0\\n00:17:24.079 of these two players is the better three-point\\xa0shooter if you were just looking at the sample\\xa0\\xa0\\n00:17:28.730 statistics here you'd say well Meyers Leonard is\\xa0right because he's got a 60% or 0.6 proportion\\xa0\\xa0\\n00:17:34.790 for three-pointers raised Steph Curry's only\\xa0shooting 0.476 so what is it about Meyers Leonard\\xa0\\xa0\\n00:17:41.270 that might tweak your intuition that something's\\xa0not quite right here well let's investigate in\\xa0\\xa0\\n00:17:47.780 a statistical way this point 6 that we've got for\\xa0Meyers Leonard is an estimate for he is Theta and\\xa0\\xa0\\n00:17:53.600 I've got this in green now this is a different\\xa0theta as the one we saw before which was for\\xa0\\xa0\\n00:17:58.280 Steph Curry but this is Maya's Leonard's long term\\xa0three-point percentage and the best estimate for\\xa0\\xa0\\n00:18:06.110 that again is our sample estimate which is 0.6 ooo\\xa0but in this case we might find that the confidence\\xa0\\xa0\\n00:18:15.110 interval we create is a lot larger for Meyers\\xa0Leonard than it is for Steph Curry why is it a lot\\xa0\\xa0\\n00:18:22.010 larger for Meyers Leonard well because he's only\\xa0had 15 three-point attempts in the season so far\\xa0\\xa0\\n00:18:26.870 so we're going to be less sure about where this\\xa0value of theta is going to be for Meyers Leonard\\xa0\\xa0\\n00:18:35.060 but again we can construct here's 95% confidence\\xa0interval which is going to be a lot wider than\\xa0\\xa0\\n00:18:41.060 Steph Curry's because we actually had more\\xa0information for Steph Curry so if you put both\\xa0\\xa0\\n00:18:47.450 of these two side-by-side the red being Steph\\xa0Curry and the green being Meyers Leonard it's\\xa0\\xa0\\n00:18:54.560 true that if we didn't know anything about these\\xa0two players we'd still have the best estimate for\\xa0\\xa0\\n00:19:00.650 Meyers Leonard being higher than for Steph Curry\\xa0but you can see we'd have a much larger confidence\\xa0\\xa0\\n00:19:06.710 interval for Meyers Leonard in other words would\\xa0be less confident about where his long-term\\xa0\\xa0\\n00:19:12.290 3-point percentage is going to be and it could be\\xa0down here below Steph Curry's and knowing what we\\xa0\\xa0\\n00:19:19.970 do about the two players it's probably likely\\xa0to be less than Steph Curry's so again this is\\xa0\\xa0\\n00:19:28.100 preparing you for what statistics can do which\\xa0is deal and quantify uncertainty now we've met\\xa0\\xa0\\n00:19:36.290 theta just a second ago which is this long term\\xa0three-point percentage but when I described it as\\xa0\\xa0\\n00:19:43.100 a Greek letter I was referring to it essentially\\xa0as what we call a parameter now let's have a look\\xa0\\xa0\\n00:19:49.430 at some common parameters that we're gonna see\\xa0in the study of statistics you might have heard\\xa0\\xa0\\n00:19:55.040 of some of these mu is often used for the mean\\xa0of a numerical variable so for example the main\\xa0\\xa0\\n00:20:01.640 height of players might be given mu Sigma is the\\xa0standard deviation of a numerical variable now\\xa0\\xa0\\n00:20:10.520 I haven't dealt with standard deviation in this\\xa0video but all standard deviation is is a measure\\xa0\\xa0\\n00:20:15.860 of the variation a measure of the uncertainty\\xa0of a particular estimate or the variation of a\\xa0\\xa0\\n00:20:22.310 particular distribution another parameter PI these\\xa0are all Greek letters by the way if you haven't\\xa0\\xa0\\n00:20:28.910 noticed pi is for the proportion of a categorical\\xa0variable so I could have used PI in that example\\xa0\\xa0\\n00:20:34.460 I just gave I ended up using theta and as I say\\xa0down the bottom here theta is generally used for\\xa0\\xa0\\n00:20:41.000 all parameters in some texts and I like using\\xa0theta because it sort of is a bit more general\\xa0\\xa0\\n00:20:47.750 but pi is sometimes used for the proportion Rho\\xa0is used when you're dealing with the correlation\\xa0\\xa0\\n00:20:54.560 between two variables and beta is used for the\\xa0gradient between two variables and that's often\\xa0\\xa0\\n00:21:00.770 used in regression which is a very important topic\\xa0in statistics and one for which I've put together\\xa0\\xa0\\n00:21:08.720 a whole series of videos so you can investigate\\xa0the videos I've done on regression if you like\\xa0\\xa0\\n00:21:15.110 now again all these represent parameters all those\\xa0unknowable fixed values that we try to estimate\\xa0\\xa0\\n00:21:24.650 now they themselves do not have any uncertainty\\xa0about them technically they are these godly\\xa0\\xa0\\n00:21:32.990 figures that we just try to merely estimate as\\xa0mere statisticians and the way we estimate them is\\xa0\\xa0\\n00:21:39.260 by taking a sample and those sample statistics are\\xa0given other symbols for a numerical variable say\\xa0\\xa0\\n00:21:46.760 height if we're taking the average height off a\\xa0sample that gets given the simple x-bar a standard\\xa0\\xa0\\n00:21:54.950 deviation is given the symbol s P is generally\\xa0used for proportion R for correlation and B for\\xa0\\xa0\\n00:22:02.420 the gradient so be prepared to see all of these\\xa0particular lowercase Roman numerals to represent\\xa0\\xa0\\n00:22:09.920 the sample values that estimate these parameters\\xa0provided in Greek but I will say be prepared also\\xa0\\xa0\\n00:22:18.710 for your statistics textbook to break all of those\\xa0rules because this despite them being conventions\\xa0\\xa0\\n00:22:25.940 sometimes you'll find they don't stick to them\\xa0annoyingly all right so with that under our belt\\xa0\\xa0\\n00:22:33.770 let's go and have a look at a very common topic\\xa0in statistics called hypothesis testing now I'm\\xa0\\xa0\\n00:22:42.530 gonna start you off with an example rather than\\xa0give you some kind of hypothetical definition\\xa0\\xa0\\n00:22:47.720 here but using the data we've just seen is there\\xa0enough evidence to suggest that Maya's Leonard\\xa0\\xa0\\n00:22:54.620 is shooting above 50% so let's review his stats\\xa0again he's got nine three-pointers made out of\\xa0\\xa0\\n00:23:02.180 fifth Dean and that's 0.6 so yeah sure his sample\\xa0is greater than 50% point 5 but is that suggesting\\xa0\\xa0\\n00:23:12.570 to us that his long-term 3-point performance\\xa0is going to be above point 5 well that is a\\xa0\\xa0\\n00:23:20.550 question worthy of a hypothesis test so as we saw\\xa0in the previous section there's going to be some\\xa0\\xa0\\n00:23:27.960 variation or variance around this estimate point\\xa0600 it's not as if that's going to definitely\\xa0\\xa0\\n00:23:34.590 be his long-term 3-point proportion so what\\xa0statisticians like to do is they like to set this\\xa0\\xa0\\n00:23:43.140 thing called a null hypothesis and it's given the\\xa0expression H naught and here we're gonna set the\\xa0\\xa0\\n00:23:51.000 null hypothesis to be that Myers Leonard's long\\xa0term three-point percentage is less than or equal\\xa0\\xa0\\n00:23:56.910 to 50% less than or equal to 0.5 now why might we\\xa0do that well as a statistician we're always very\\xa0\\xa0\\n00:24:04.980 conservative we assume that the reverse is true\\xa0and then see if there's enough evidence to really\\xa0\\xa0\\n00:24:11.070 budge from that assumption it's kind of like when\\xa0someone's on trial the null hypothesis might be\\xa0\\xa0\\n00:24:17.730 that they're innocent and you really need a lot\\xa0of evidence to budge from that null hypothesis\\xa0\\xa0\\n00:24:23.970 it's not good enough that there's just a little\\xa0bit of evidence you really need evidence beyond\\xa0\\xa0\\n00:24:29.550 reasonable doubt right and that's the same with\\xa0hypothesis tests so this here on the right-hand\\xa0\\xa0\\n00:24:36.390 side is called the alternate hypothesis and in\\xa0general whenever we're doing a hypothesis test\\xa0\\xa0\\n00:24:42.030 in statistics whatever we're seeking evidence\\xa0for goes in the alternate hypothesis just\\xa0\\xa0\\n00:24:50.100 for the reason that we're very conservative as\\xa0statisticians we're always going to have a null\\xa0\\xa0\\n00:24:56.160 hypothesis that the reverse is in fact true and\\xa0we're gonna see if our sample is extreme enough\\xa0\\xa0\\n00:25:02.310 is far enough away from that null hypothesis to\\xa0suggest that the alternative hypothesis might be\\xa0\\xa0\\n00:25:10.530 true this is the way you're going to be framing\\xa0your thinking when you're dealing with statistics\\xa0\\xa0\\n00:25:15.710 now one thing that different texts different\\xa0textbooks will do will have different ways\\xa0\\xa0\\n00:25:20.960 of describing and null hypothesis they both\\xa0mean the same thing but some will say theta\\xa0\\xa0\\n00:25:27.770 is less than or equal to 0.5 and others might\\xa0say something like theta is equal to 0.5 and\\xa0\\xa0\\n00:25:34.070 it doesn't much matter because the important\\xa0thing is that theta being greater than 0.5\\xa0\\xa0\\n00:25:40.610 is in our alternate hypothesis so let's see how\\xa0this pans out using what we understand now from\\xa0\\xa0\\n00:25:48.950 a probability distribution so essentially what\\xa0we're going to do is we're gonna start with this\\xa0\\xa0\\n00:25:53.720 null hypothesis that theta is equal to 0.5 and if\\xa0it indeed is equal to 0.5 how many three-pointers\\xa0\\xa0\\n00:26:01.730 out of 15 would Meyers Leonard sink well here's\\xa0the probability distribution if he truly is a 50%\\xa0\\xa0\\n00:26:10.460 3-point shooter and exactly 50% 3-point shooter\\xa0if he shoots 15 three-pointers on average he's\\xa0\\xa0\\n00:26:19.280 going to get 7.5 of those in right but of course\\xa0you can't sink exactly 7.5 so 7 & 8 they'll be\\xa0\\xa0\\n00:26:27.650 approximately the same height so they'll have the\\xa0same probability of occurring he's less likely to\\xa0\\xa0\\n00:26:32.750 get 6 and 9 less likely again to get 5 and 10 etc\\xa0etc etc so this is the probability distribution\\xa0\\xa0\\n00:26:39.890 of Meyers Leonard's 15 three-point attempts where\\xa0the number of successes are on this axis assuming\\xa0\\xa0\\n00:26:47.840 the null hypothesis is true now for those advanced\\xa0players this is actually a binomial distribution\\xa0\\xa0\\n00:26:53.990 if you're keen on learning more I'll put a link\\xa0up here now what did he get in this sample well\\xa0\\xa0\\n00:27:01.190 he actually got 9 so what this tells us is that if\\xa0indeed he has a 50% 3-point percentage it's still\\xa0\\xa0\\n00:27:09.380 quite likely for him to get nine three-pointers\\xa0out of 15 it's not beyond the realm of possibility\\xa0\\xa0\\n00:27:16.940 that he could be a 50% 3-point shooter and just\\xa0happened to do a little bit better in his first\\xa0\\xa0\\n00:27:22.130 15 shots than expected now if I was to ask you\\xa0how much doubt this sample is casting our null\\xa0\\xa0\\n00:27:29.600 hypothesis in this case you'd say well not very\\xa0much what if I then told you that say someone else\\xa0\\xa0\\n00:27:36.890 scored 12 out of 15 three-pointers well let's\\xa0just say Myers scored 12 out of 15 instead at\\xa0\\xa0\\n00:27:43.820 that point you're starting to think you know what\\xa0that's quite unlikely now it's still possible that\\xa0\\xa0\\n00:27:49.970 he's truly a 50% 3-point shooter and managed to\\xa0just do better in his first 15 than we expected\\xa0\\xa0\\n00:27:57.020 but it's starting to now cast some doubt on our\\xa0null hypothesis and this is what a hypothesis\\xa0\\xa0\\n00:28:04.310 test does it takes the sample and says how extreme\\xa0is that sample is it too extreme given our null\\xa0\\xa0\\n00:28:11.270 hypothesis for us to realistically hold on to\\xa0that null hypothesis so in reality what's going\\xa0\\xa0\\n00:28:19.100 to happen is we're going to construct what's\\xa0called a rejection region we'll find a point\\xa0\\xa0\\n00:28:24.440 on this x-axis here beyond which we're going to\\xa0consider it too extreme to realistically hold on\\xa0\\xa0\\n00:28:31.160 to the null hypothesis being true now this yellow\\xa0area can effectively be customized to determine\\xa0\\xa0\\n00:28:39.230 how strict you wanna be with rejecting this null\\xa0hypothesis but often it's chosen as 5% of the\\xa0\\xa0\\n00:28:46.880 entire distribution and we call this the level\\xa0of significance so we might say that the level\\xa0\\xa0\\n00:28:52.910 of significance here is 5% because if our sample\\xa0statistic is in this upper 5% we will consider\\xa0\\xa0\\n00:28:59.480 it too extreme for the null hypothesis\\xa0and therefore reject the null hypothesis\\n00:29:07.370 so just to repeat in this case because Maya's\\xa0Leonard got 9 out of 15 on point 6 he was in this\\xa0\\xa0\\n00:29:15.890 point here he was at 9 therefore not extreme\\xa0enough to reject the null hypothesis so even\\xa0\\xa0\\n00:29:23.420 though in the sample he was shooting above\\xa050% it wasn't extreme enough to allow us to\\xa0\\xa0\\n00:29:28.970 infer that he's shooting 50% in the long-term we\\xa0need more evidence as conservative statisticians\\n00:29:38.340 anyway so I just got a little extra look section\\xa0here for hypothesis testing just for you to be\\xa0\\xa0\\n00:29:45.750 aware of two important notes the first thing is\\xa0that we never ever prove anything in a hypothesis\\xa0\\xa0\\n00:29:54.510 test so here again is that set up with Meyers\\xa0Leonard and our conclusion which was to not reject\\xa0\\xa0\\n00:30:00.870 the null hypothesis as in there's not enough\\xa0evidence to suggest my as Leonard is shooting\\xa0\\xa0\\n00:30:05.700 above 50% never say the word prove in your\\xa0conclusion which is the frustrating thing about\\xa0\\xa0\\n00:30:13.230 statistics I guess you can never prove anything\\xa0at all or you can do is infer so we were unable\\xa0\\xa0\\n00:30:19.650 to infer that Meyers Leonard is shooting above 50%\\xa0the other thing we never say is the word accept so\\xa0\\xa0\\n00:30:28.980 notice I've written it and do not reject the null\\xa0hypothesis so this was our null but in the event\\xa0\\xa0\\n00:30:35.280 that we do not reject it you should never say the\\xa0word we then accept the null hypothesis because\\xa0\\xa0\\n00:30:41.700 don't forget he scored 60% out of the first\\xa015 three-pointers so it's not as if we have\\xa0\\xa0\\n00:30:48.630 evidence that he's less than a 50% 3-point shooter\\xa0it's just that we don't have evidence that he's\\xa0\\xa0\\n00:30:56.700 more than a 50% 3-point shooter that's a really\\xa0important distinction in fact a whole judicial\\xa0\\xa0\\n00:31:03.720 system relies on that distinction when you find\\xa0someone not guilty that doesn't necessarily mean\\xa0\\xa0\\n00:31:11.160 that they're innocent right it just means that\\xa0there's not been enough evidence to convince you\\xa0\\xa0\\n00:31:15.870 of their guilt and because of the presumption\\xa0of innocence they walk free okay so let's have\\xa0\\xa0\\n00:31:26.490 a look at p-values now they're the much-maligned\\xa0p-values in statistics now to introduce them i've\\xa0\\xa0\\n00:31:34.890 said considering a null hypothesis so whatever\\xa0null hypothesis were testing hypothesis tests\\xa0\\xa0\\n00:31:42.930 assess if our sample is extreme enough to reject\\xa0the null hypothesis that's exactly what we did in\\xa0\\xa0\\n00:31:49.470 the last section what the p-value does is that\\xa0then measures how extreme the sample is so the\\xa0\\xa0\\n00:31:58.110 hypothesis tests sort of set up the goal posts and\\xa0we assess whether we've scored the goal or not but\\xa0\\xa0\\n00:32:03.930 the p-value measures out exactly how far we kicked\\xa0the ball to continue with a fairly loose analogy\\xa0\\xa0\\n00:32:10.770 there so here's the example again we're using the\\xa0same setup as before with Meyers Leonard's 50%\\xa0\\xa0\\n00:32:18.630 three-point percentage so our test statistic was\\xa09 so he got 9 out of 15 three-pointers right and\\xa0\\xa0\\n00:32:26.100 this is the distribution under the null hypothesis\\xa0so how extreme was his test statistic that we got\\xa0\\xa0\\n00:32:33.120 well we found out it wasn't extreme enough right\\xa0so the hypothesis test said reject the null\\xa0\\xa0\\n00:32:39.450 hypothesis if the test statistic is in the top\\xa05% of the distribution and indeed we found that\\xa0\\xa0\\n00:32:46.710 he was not in the top 5% of the distribution what\\xa0the p-value does is it takes our test statistic\\xa0\\xa0\\n00:32:53.490 and actually calculates that region so it says our\\xa0test statistic is in the top 30 point 4 percent of\\xa0\\xa0\\n00:33:00.240 the distribution 0.304 so it's actually measuring\\xa0how much of the distribution is at or above our\\xa0\\xa0\\n00:33:08.610 test statistic so in other words it's measuring\\xa0how extreme our sample is so if our p-value is\\xa0\\xa0\\n00:33:17.610 very small the more extreme our sample must have\\xa0been and therefore the more likely we are to\\xa0\\xa0\\n00:33:23.760 reject the null hypothesis and if the p-value\\xa0is large we're less likely to reject the null\\xa0\\xa0\\n00:33:30.570 hypothesis so if it's closer to one it's this one\\xa0is this was 0.3 so we had quite a large pink area\\xa0\\xa0\\n00:33:37.590 here we become less likely to reject the null\\xa0hypothesis and that's exactly what happened in\\xa0\\xa0\\n00:33:42.750 our case we did not reject our null hypothesis now\\xa0the final point I might make and it's something\\xa0\\xa0\\n00:33:50.250 that you probably have figured out already maybe\\xa0but if this p-value drops below 0.05 it implies\\xa0\\xa0\\n00:33:57.810 that our test statistic must be in the rejection\\xa0region let me repeat that if the p-value is less\\xa0\\xa0\\n00:34:05.100 point O five it means that our test statistic\\xa0wherever we are must be in the rejection region\\xa0\\xa0\\n00:34:11.400 so that rejection region was constructed that\\xa0yellow bit let's go back that yellow bit was\\xa0\\xa0\\n00:34:17.190 constructed so there's five percent that's been\\xa0highlighted five percent of the whole distribution\\xa0\\xa0\\n00:34:21.690 that's been highlighted so if our p-value is\\xa0less than five percent or less than 0.05 if\\xa0\\xa0\\n00:34:28.710 the pink bit was less than 0.05 we know that we\\xa0must be somewhere in that rejection region our\\xa0\\xa0\\n00:34:35.610 test statistic must be in the rejection region so\\xa0what that implies is if the p-value is less than\\xa0\\xa0\\n00:34:44.730 the level of significance for your hypothesis\\xa0test you're going to reject the null hypothesis\\xa0\\xa0\\n00:34:50.969 so it's a really quick way of assessing whether\\xa0we're going to be rejecting our null hypothesis\\xa0\\xa0\\n00:34:57.270 right so all up whenever you conduct a hypothesis\\xa0test let's sort of recap whatever you're seeking\\xa0\\xa0\\n00:35:04.530 evidence for goes in your alternate hypothesis and\\xa0then if you conduct the test and your p-value is\\xa0\\xa0\\n00:35:11.550 very very small that provides evidence for that\\xa0alternate hypothesis it provides evidence enough\\xa0\\xa0\\n00:35:20.520 for us to reject the null hypothesis all right\\xa0so that's pretty much it for the theoretical\\xa0\\xa0\\n00:35:27.180 component of this video stop the clock did I\\xa0get under 30 minutes I don't think so I think\\xa0\\xa0\\n00:35:33.360 it was a few minutes over but I'm not even going\\xa0to quiet stop the video here because I figured I\\xa0\\xa0\\n00:35:39.000 might give you an extra little section to do with\\xa0p-values because they've been in the news over the\\xa0\\xa0\\n00:35:45.090 last few years I would say and not necessarily\\xa0in a good way people have been throwing a lot\\xa0\\xa0\\n00:35:51.180 of shade at scientific research over the last\\xa0little while and it's somewhat justified due to\\xa0\\xa0\\n00:35:56.610 this thing called P hacking so if you've had\\xa0enough with the theoretical component of the\\xa0\\xa0\\n00:36:01.350 stats today well I'll tell you that's it we're\\xa0done but let's have a look at P hacking to see\\xa0\\xa0\\n00:36:06.840 how a misuse of p-values can invalidate scientific\\xa0research so let's talk about what P hackings all\\xa0\\xa0\\n00:36:16.130 about and I might start with that same boring old\\xa0probability density function that we saw before\\xa0\\xa0\\n00:36:21.830 now as we've seen in hypothesis testing we start\\xa0with the null hypothesis that there's no effect\\xa0\\xa0\\n00:36:29.270 and then we take a sample and we want to see a\\xa0sample that's extreme enough for us to reject that\\xa0\\xa0\\n00:36:36.110 null hypothesis so we'll construct this rejection\\xa0region which is this yellow shaded region up here\\xa0\\xa0\\n00:36:42.620 my choice of colors might not have been the best\\xa0but hopefully you can see that's shaded yellow\\xa0\\xa0\\n00:36:47.780 and so if our sample lies up in this region here\\xa0we're able to reject the null hypothesis and in\\xa0\\xa0\\n00:36:55.610 doing so we would say that there's a significant\\xa0effect and all of a sudden that's great we'll be\\xa0\\xa0\\n00:37:00.440 able to publish our paper to show that X affects\\xa0Y and we'll get all the plaudits from the research\\xa0\\xa0\\n00:37:07.310 community but here's the thing remember how I said\\xa0that statistics doesn't prove anything well this\\xa0\\xa0\\n00:37:15.200 is exactly the case if we have a sample which is\\xa0in our rejection region in other words a sample\\xa0\\xa0\\n00:37:21.710 which is extreme enough for us to reject the null\\xa0hypothesis it doesn't mean the null hypothesis is\\xa0\\xa0\\n00:37:28.460 false it's still possible that we just happen to\\xa0get a freak sample right the whole purpose of a\\xa0\\xa0\\n00:37:36.530 p-value is to say well how likely is it for us to\\xa0get this sample statistic if the null hypothesis\\xa0\\xa0\\n00:37:44.420 is true and if the p-value is low enough we go oh\\xa0that's starting to become too low but at the same\\xa0\\xa0\\n00:37:51.350 time as long as that p-values nonzero there is an\\xa0outside chance but you just happen to get a freak\\xa0\\xa0\\n00:37:59.150 sample where there was in fact no effect to put it\\xa0in the basketball terms just say my as Leonard was\\xa0\\xa0\\n00:38:07.940 a 50% 3-point shooter it's still possible for\\xa0him to score 14 or 15 out of 15 three-pointers\\xa0\\xa0\\n00:38:15.620 right very unlikely if his true 3-point percentage\\xa0was 50% but it's possible so how does this relate\\xa0\\xa0\\n00:38:26.560 to good and bad research well in good research\\xa0what you do is you theorize some kind of effect\\xa0\\xa0\\n00:38:34.540 and maybe that might be that red wine causes\\xa0cancer let's just say that as an example right\\xa0\\xa0\\n00:38:39.880 we then collect our data and we test only that\\xa0effect red wine causing cancer and if we find\\xa0\\xa0\\n00:38:50.620 the p-value of this test less than 0.05 we can\\xa0conclude some strong evidence for the effect\\xa0\\xa0\\n00:38:56.710 of red wine on cancer and that's all well and\\xa0good and that's good research that process of\\xa0\\xa0\\n00:39:02.830 theorizing some effect then collecting your data\\xa0and testing that exact effect is how one conducts\\xa0\\xa0\\n00:39:10.570 good research now bad research gets conducted\\xa0like this and unfortunately I'm gonna suggest\\xa0\\xa0\\n00:39:18.790 this gets done all the time if you collect your\\xa0data first with just the general idea of let's\\xa0\\xa0\\n00:39:25.960 see what causes cancer so let's collect a whole\\xa0bunch of data from people that have cancer a lot\\xa0\\xa0\\n00:39:31.450 of lifestyle kinds of pieces of data as well\\xa0whether they smoke whether they drink wine all\\xa0\\xa0\\n00:39:37.060 this kind of stuff we're gonna test all these\\xa0different effects we're gonna test red wine\\xa0\\xa0\\n00:39:40.960 we're gonna test smoking we're gonna test exercise\\xa0we're gonna test exposure two main roads all this\\xa0\\xa0\\n00:39:48.730 kind of stuff and then we're gonna look through\\xa0all those effects and find the ones where P is\\xa0\\xa0\\n00:39:52.930 less than 0.05 but let's just say it happened to\\xa0be four where we're testing red wine on cancer\\xa0\\xa0\\n00:40:00.970 and then we're gonna publish our results and say\\xa0yep red wine causes cancer because the p-value\\xa0\\xa0\\n00:40:06.100 is less than 0.5 this is called pee hacking and\\xa0is potentially rife in research and it's quite\\xa0\\xa0\\n00:40:14.950 problematic now it's not necessarily obvious why\\xa0this is so much worse than our good research over\\xa0\\xa0\\n00:40:21.130 here on the left but as I said before when\\xa0we conclude strong evidence for some effect\\xa0\\xa0\\n00:40:27.940 we're essentially saying there's a very very low\\xa0probability that this came about by chance now\\xa0\\xa0\\n00:40:35.950 what happens when you test 10 different things\\xa0if you test 10 different things it becomes more\\xa0\\xa0\\n00:40:42.610 likely that one of them by chance will be quite\\xa0extreme in their sampling well let's push it even\\xa0\\xa0\\n00:40:49.150 further if you test 20 different things we're\\xa0actually expecting 1 out of those 20 things to\\xa0\\xa0\\n00:40:56.890 have a p-value less than 0.05 that's actually\\xa0what the p-value means if there's a 5% chance\\xa0\\xa0\\n00:41:04.780 that the effect we've seen was just due to the\\xa0randomness of the sampling process than if we test\\xa0\\xa0\\n00:41:10.630 20 things 5% of 20 is 1 one of those 20 things is\\xa0likely to show that strength of effect so that's\\xa0\\xa0\\n00:41:20.200 where he hacking comes into it we test all these\\xa0different things and we just find the one that\\xa0\\xa0\\n00:41:25.270 happens to look significant and we can then sort\\xa0of pretend that that was the thing that we were\\xa0\\xa0\\n00:41:31.120 looking for the whole time and it's actually a\\xa0big big problem anyway that's hopefully brought\\xa0\\xa0\\n00:41:37.630 brought it into practice some of the stuff that\\xa0you can learn in statistics and of course I've\\xa0\\xa0\\n00:41:42.640 dealt with things in a very superficial way but\\xa0that was the whole point of this video but look if\\xa0\\xa0\\n00:41:48.790 you like this I've got more in-depth discussions\\xa0one where I go into the actual formula and the\\xa0\\xa0\\n00:41:54.670 mathematics of it all you can check it all out\\xa0on Zed statistics com but hey if you dig it you\\xa0\\xa0\\n00:42:00.130 can like and subscribe and do all those things\\xa0that you're meant to do but yeah hope you enjoyed\\n\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Replace with your actual file path\n",
    "file_path = \"/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt\"\n",
    "\n",
    "# Initialize the loader\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "# Load the document\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a4536",
   "metadata": {},
   "source": [
    "Split Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc3a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1f10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='00:00:00.000 Hey team, Justin Zeltzer here from zstatistics.com, where today I'm  \n",
      "00:00:05.430 responding to a challenge that was issued to me. Someone asked me if I could explain statistics  \n",
      "00:00:11.160 to them in under half an hour. While initially I thought that was a bit of an ambitious ask,  \n",
      "00:00:17.250 I thought no that's actually a really good challenge, and one that I might do for everybody.  \n",
      "00:00:23.280 So this is it! An introduction to statistics, with no maths, and done in under half an hour.  \n",
      "00:00:30.300 Now you can probably see that the the timing of this video is a bit longer than that, but it is  \n",
      "00:00:36.480 because I bunged on a little extra section at the end- which is a bit of an optional extra,  \n",
      "00:00:41.220 but I think I get most of it done in under half an hour. But the idea is for you to develop your  \n",
      "00:00:47.280 intuition around statistics, so it's great for those people who are just enrolling in a' metadata={'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081abf87",
   "metadata": {},
   "source": [
    "Move TimeStamps into MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a44e3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt', 'start_time': '00:00:00.000', 'end_time': '00:00:47.280'}, page_content=\"Hey team, Justin Zeltzer here from\\xa0zstatistics.com, where today I'm\\xa0\\xa0\\n responding to a challenge that was issued to me.\\xa0Someone asked me if I could explain statistics\\xa0\\xa0\\n to them in under half an hour. While initially\\xa0I thought that was a bit of an ambitious ask,\\xa0\\xa0\\n I thought no that's actually a really good\\xa0challenge, and one that I might do for everybody.\\xa0\\xa0\\n So this is it! An introduction to statistics,\\xa0with no maths, and done in under half an hour.\\xa0\\xa0\\n Now you can probably see that the the timing of\\xa0this video is a bit longer than that, but it is\\xa0\\xa0\\n because I bunged on a little extra section at\\xa0the end- which is a bit of an optional extra,\\xa0\\xa0\\n but I think I get most of it done in under half\\xa0an hour. But the idea is for you to develop your\\xa0\\xa0\\n intuition around statistics, so it's great\\xa0for those people who are just enrolling in a\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Regex to match timestamps like 00:01:05.438\n",
    "timestamp_pattern = r\"\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{3})?\"\n",
    "\n",
    "updated_docs = []\n",
    "\n",
    "for doc in splits:\n",
    "    text = doc.page_content\n",
    "\n",
    "    # Find all timestamps in this chunk\n",
    "    timestamps = re.findall(timestamp_pattern, text)\n",
    "\n",
    "    # strip all timestamps from the text\n",
    "    cleaned_text = re.sub(timestamp_pattern, '', text).strip()\n",
    "\n",
    "    # extract the first and last timestamps (if any)\n",
    "    start_time = timestamps[0] if timestamps else None\n",
    "    end_time = timestamps[-1] if timestamps else None\n",
    "\n",
    "    # add timestamps to metadata\n",
    "    updated_metadata = dict(doc.metadata)  # copy existing metadata\n",
    "    updated_metadata[\"start_time\"] = start_time\n",
    "    updated_metadata[\"end_time\"] = end_time\n",
    "\n",
    "    \n",
    "    updated_docs.append(Document(page_content=cleaned_text, metadata=updated_metadata))\n",
    "\n",
    "updated_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904af3df",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c371ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99cf6bf",
   "metadata": {},
   "source": [
    "Delete old Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327d5292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: chroma.sqlite3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "persist_directory = \"/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final\"\n",
    "\n",
    "# Chroma-specific files/folders to delete\n",
    "files_to_delete = [\n",
    "    \"chroma.sqlite3\"\n",
    "]\n",
    "\n",
    "for item in files_to_delete:\n",
    "    path = os.path.join(persist_directory, item)\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "        print(f\"Deleted file: {item}\")\n",
    "    elif os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Deleted folder: {item}\")\n",
    "    else:\n",
    "        print(f\"Not found: {item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d978f1f",
   "metadata": {},
   "source": [
    "Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc8511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd58023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/ysm8yfcn76s_32xvcxfbp8s00000gn/T/ipykernel_12983/801237729.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d75b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04576ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents = updated_docs,\n",
    "    embedding = embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e2115",
   "metadata": {},
   "source": [
    "LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27111ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d70924",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1a68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/ysm8yfcn76s_32xvcxfbp8s00000gn/T/ipykernel_12983/1529095479.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name = llm_name, temperature = 0, openai_api_key = openai.api_key)\n"
     ]
    }
   ],
   "source": [
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_name, temperature = 0, openai_api_key = openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4000071",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end\n",
    "If you don't know the answer, just say \"I don't know\". For the response, use three sentences max.\n",
    "Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end\n",
    "of the answer.\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "498a72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever = vectordb.as_retriever(),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "024fc0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/ysm8yfcn76s_32xvcxfbp8s00000gn/T/ipykernel_12983/3556828445.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\" : question})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hypothesis testing is a common topic in statistics where we seek evidence to support an alternate hypothesis by comparing it to a null hypothesis. The p-value in hypothesis testing measures how extreme our sample data is, helping us determine whether to reject the null hypothesis. Thanks for asking!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Hypothesis Testing\"\n",
    "result = qa_chain({\"query\" : question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1527f9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt', 'start_time': '00:21:54.950', 'end_time': '00:22:47.720'}, page_content=\"deviation is given the symbol s P is generally\\xa0used for proportion R for correlation and B for\\xa0\\xa0\\n the gradient so be prepared to see all of these\\xa0particular lowercase Roman numerals to represent\\xa0\\xa0\\n the sample values that estimate these parameters\\xa0provided in Greek but I will say be prepared also\\xa0\\xa0\\n for your statistics textbook to break all of those\\xa0rules because this despite them being conventions\\xa0\\xa0\\n sometimes you'll find they don't stick to them\\xa0annoyingly all right so with that under our belt\\xa0\\xa0\\n let's go and have a look at a very common topic\\xa0in statistics called hypothesis testing now I'm\\xa0\\xa0\\n gonna start you off with an example rather than\\xa0give you some kind of hypothetical definition\\xa0\\xa0\\n here but using the data we've just seen is there\\xa0enough evidence to suggest that Maya's Leonard\"),\n",
       " Document(metadata={'start_time': '00:24:29.550', 'end_time': '00:25:20.960', 'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt'}, page_content=\"reasonable doubt right and that's the same with\\xa0hypothesis tests so this here on the right-hand\\xa0\\xa0\\n side is called the alternate hypothesis and in\\xa0general whenever we're doing a hypothesis test\\xa0\\xa0\\n in statistics whatever we're seeking evidence\\xa0for goes in the alternate hypothesis just\\xa0\\xa0\\n for the reason that we're very conservative as\\xa0statisticians we're always going to have a null\\xa0\\xa0\\n hypothesis that the reverse is in fact true and\\xa0we're gonna see if our sample is extreme enough\\xa0\\xa0\\n is far enough away from that null hypothesis to\\xa0suggest that the alternative hypothesis might be\\xa0\\xa0\\n true this is the way you're going to be framing\\xa0your thinking when you're dealing with statistics\\xa0\\xa0\\n now one thing that different texts different\\xa0textbooks will do will have different ways\\xa0\\xa0\\n of describing and null hypothesis they both\\xa0mean the same thing but some will say theta\"),\n",
       " Document(metadata={'end_time': '00:32:39.450', 'source': '/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/stats1 transcript.txt', 'start_time': '00:31:49.470'}, page_content=\"the last section what the p-value does is that\\xa0then measures how extreme the sample is so the\\xa0\\xa0\\n hypothesis tests sort of set up the goal posts and\\xa0we assess whether we've scored the goal or not but\\xa0\\xa0\\n the p-value measures out exactly how far we kicked\\xa0the ball to continue with a fairly loose analogy\\xa0\\xa0\\n there so here's the example again we're using the\\xa0same setup as before with Meyers Leonard's 50%\\xa0\\xa0\\n three-point percentage so our test statistic was\\xa09 so he got 9 out of 15 three-pointers right and\\xa0\\xa0\\n this is the distribution under the null hypothesis\\xa0so how extreme was his test statistic that we got\\xa0\\xa0\\n well we found out it wasn't extreme enough right\\xa0so the hypothesis test said reject the null\\xa0\\xa0\\n hypothesis if the test statistic is in the top\\xa05% of the distribution and indeed we found that\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search(question, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0f247",
   "metadata": {},
   "source": [
    "download youtube video to computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd7e5f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "from moviepy import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b645f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_seconds(ts):\n",
    "    h, m, s = ts.split(\":\")\n",
    "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "\n",
    "def ask_question(question, input_path): \n",
    "    result = qa_chain({\"query\": question})\n",
    "    result_text = result['result']\n",
    "    print(\"Output:\", result_text)\n",
    "    \n",
    "    source = result['source_documents']\n",
    "    source_md = source[0].metadata\n",
    "    start_time = source_md['start_time']\n",
    "    end_time = source_md['end_time']\n",
    "    \n",
    "    start_sec = timestamp_to_seconds(start_time)\n",
    "    end_sec = timestamp_to_seconds(end_time)\n",
    "    \n",
    "    clip = VideoFileClip(input_path).subclipped(start_sec, end_sec)\n",
    "    clip.write_videofile(f\"output_clip.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "    \n",
    "    print(\"Clip:\\n\")\n",
    "    Video(\"output_clip.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f9e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Hypothesis testing is a statistical method used to determine if there is enough evidence to support a claim or hypothesis. It involves setting up a null hypothesis (the opposite of what is being tested) and an alternate hypothesis (what is being tested for), and then analyzing data to see if the null hypothesis can be rejected. Thanks for asking!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video output_clip.mp4.\n",
      "MoviePy - Writing audio in output_clipTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/1134 [00:00<?, ?it/s, now=None]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video output_clip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready output_clip.mp4\n",
      "Clip:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/Users/emiliodulay/Documents/DSU/Study Buddy - Stats Final/intro stats.mp4\"\n",
    "\n",
    "ask_question(question = \"Can you explain what hypothesis testing is?\",\n",
    "            input_path = video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf27aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
